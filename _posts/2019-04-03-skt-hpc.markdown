---
layout: post
title: "СКТ. Высокопроизводительные вычисления"
---

Под высокопроизводительными вычислениями обычно подразумевают практику объединения вычислительных мощностей для увеличения суммарной производительности.
В англоязычной литературе используется термин *High Performance Computing* (*HPC*).

Ресурсов персонального компьютера обычно бывает не достаточно для решения некоторых задач.
Например, для предсказания погоды.
Нужно узнать более точную погоду на завтра, имея метеоданные за последнее время.
Результат нужно получить как можно скорее.
В противном случае через 24 часа мы получим точный прогноз просто посмотрев в окно, без сложных вычислений.


Почему актуальны HPC?
+ Большой объем информации
+ Сложные методы анализа данных, увеличивающие точность и скорость решения прежних задач.
+ Требования к времени получения результата. В противном случае результат теряет актуальность.

Области применения:
+ Проектирование ядерных зарядов и наложение запрета на их испытания.
+ Численные методы решения уравнений - между физическим экспериментом и аналитическими методами. Вычислительный эксперимент дешевый, быстрый. Физический эксперимент - опасный, дорогой, невозможный.
+ Научные вычисления:
  + предсказание погоды;
  + предсказание объемов нефтедобычи;
  + вычислительная гидродинамика;
+ Организация поиска в больших массивах данных.
+ Проектирование сложной техники:
  + моделирование критических ситуаций;
  + расчеты на прочность конструкций;

Конференции, посвященные HPC -- это
+ международна научная конференция ["Параллельные вычислительные технологии"](http://agora.guru.ru/display.php?conf=pavt2019) (ПаВТ)

Пути увеличения производительности - совершенствование процессоров, архитектур компьютера, программных средств и алгоритмов (способов решения задачи).

Три технологии организации высокопроизводительных вычислений

+ вычислительный кластер &mdash; это набор вычислительных устройств, объединенных высокоскоростными каналами связи.
+ грид-система &mdash; это набор вычислительных устройств, объединенных низкоскоростными каналами связи.
+ облачные вычисления &mdash; концепция обеспечения доступа по требованию к общему пулу вычислительных ресурсов.

**Суперкомпьютер** &mdash; это специализированная вычислительная машина, значительно превосходящая по своим техническим параметрам и скорости вычислений большинство существующих в мире компьютеров. В определении не используются конкретные параметры, потому что со временем они устаревают.

**Закон Мура** &mdash; эмпирическое наблюдение, сделанное Гордоном Муром, согласно которому количество транзисторов на интегральной микросхеме удваивается каждые 24 месяца.

Сочетание роста количества транзисторов и увеличения тактовой частоты приводит к удвоению производительности каждые 18 месяцев (уточняющий прогноз Давида Хауса).

Но рост производительности процессора ограничен размерами атома и скоростью света. Размер атома кремния - 0.2 нм.

Трехмерные многослойные чипы. Уплотнение схемы, уменьшение расстояния и задержки. Но как охладить такую конструкцию?

Развитие программного обеспечения толкало аппаратное обеспечение. Но мобильные устройства изменили эту тенденцию. Программное обеспечение подстраивается под новые аппаратные возможности, имеет значение энергоэффективность. Обойти это - облачные вычисления.

# Top 500 суперкомпьютеров

Проект по составлению рейтинга 500 самых высокопроизводительных ЭВМ. Итоги подводятся 2 раза в год - июне и ноябре. Последний рейтинг можно посмотреть [здесь](https://www.top500.org/lists/2018/11/). Рейтинг составляется на основе результатов запуска теста LINPACK. В LINPACK происходит решение больших систем уравнений.

Самый мощный суперкомпьютер Summit, разработанный IBM, основан на графических процессорах от Nvidia и процессоров POWER от IBM. На испытаниях показала максимальную производительность 143 ПФлопс. Операционная система Linux.

Самый мощный суперкомпьютер в России - Ломоносов-2, производительность 2,1 ПФлопс.

# Классификация архитектур ЭВМ по Флинну

**SISD** (Single Instruction, Single Data) &mdash; ВС с одиночным потоком команд и данных. Классический компьютер фон-Неймановской архитектуры. К этому классу относят все суперскалярные, конвейерные и VLIW-процессоры.

**SIMD** (Single Instruction, Multiple Data) &mdash; одиночный поток команд и множественный поток данных. Векторные процессоры, векторное расширение процессоров (SSE), видеокарты, многопроцессорные системы (матричные процессоры).

+ SM-SIMD (shared memory SIMD) - векторные процессоры.
+ DM-SIMD (distributed memory SIMD) - матричные процессоры, графические карты.

**MISD** (Multiple Instruction, Single Data) &mdash; множественный поток команд и одиночный поток данных. Используется для резервирования вычислительных устройств для защиты от сбоя. Это космическая и военная техника.

**MIMD** (Multiple Instruction, Multiple Data) &mdash; множественный поток данных и множественный поток данных. Мультипроцессорные машины, многоядерные и многопоточные процессоры, компьютерные кластеры.

+ SM-MIMD (shared memory MIMD) - MIMD-машины с общей памятью. Отсутствие масштабируемости, чем больше процессоров в системе, тем выше нагрузка на шину данных.
  - DSM-MIMD (distributed shared memory MIMD) - у каждого процессора своя локальная память, а с другими процессорами он общается по высокоскоростным соединениям. Существует несколько типов памяти, доступ к которым имеет различные скорости - такие системы имеют название NUMA (Non uniform memory access). Противоположность этой системе - UMA (Uniform memory access).
+ DM-MIMD (distributed memory MIMD) - с распределенной памятью. Высокая масштабируемость, возможность создать массово-параллельные системы.

# Специализированные интегральные схемы для вычислений

**ПЛИС** (программируемая логическая интегральная схема, programmable logic device, PLD) &mdash; электронный компонент, используемый для создания цифровых интегральных схем. Логика работы ПЛИС определяется пользователем с помощью программирования на языках описания аппаратуры (Verilog, VHDL).

Применение - цифровая обработка сигналов, криптографические операции, проектирование и отладка специализированных микросхем и процессоров, передача данных на высоких скоростях, ускоритель универсальных процессоров в суперкомпьютерах.

**FPGA** (Field Programmable Gate Array) &mdash; одна из архитектурных разновидностей ПЛИС.

Наиболее крупные производители ПЛИС - Xilinx (произносится как зайлинкс) и Altera.

**ASIC** (Application specific integrated circuit) &mdash; интегральная микросхема, специализированная для решения конкретной задачи. Используется в конкретном устройстве и выполняет строго ограниченные функции, характерные для данного устройства.

# Развитие ахитектуры процессора

**CISC-архитектура** (Complex Instruction Set Computer) &mdash;  архитектура с полным набором команд. Процессор с такой архитектурой имеет большое количество команд на многие случаи жизни. Размер команды не фиксирован и составляет от 1 до 15 байт (для x86). Количество тактов, за которое выполнится одна команда также зависит от ее типа. Данная архитектура самая распространенная в мире и характерна для процессоров на основе системы команд x86. Команды добавлялись в процессор для расширения его функциональности и увеличения удобства программирования. Но недостатками такого развития стали высокое энергопотребление, сложные подходы к распараллеливанию, сложность внутренней структуры процессора. Одним из путей решения этих проблем стало появление RISC-архитектуры.

**RISC-архитектура** (Reduced Instruction Set Computer) &mdash; архитектура с сокращенным набором команд. Быстродействие достигается за счет упрощения системы команд. При этом сокращается не количество команд, а их размер и время выполнения. Доступ к памяти упрощается до двух команд - `load` и `store`. Декодирование простых команд упрощается и их проще конвейеризировать, что позволяет распараллеливать команды между несколькими исполнительными блоками. Поздние процессоры x86 хоть и являются CISC-совместимыми, но реализованы на базе RISC-команд. Каждая сложная команда разбивается на набор более простых, которые и выполняются. Также данной архитектурой обладают процессоры ARM, которые повсеместно используются в мобильных устройствах, процессоры POWER от IBM.

**Суперскалярный процессор** &mdash; процессор, способный выполнить несколько команд одновременно. Это достигается дублированием функциональных блоков (АЛУ, математический сопроцессор, сдвигающие устройства). Планирование исполнения команд осуществляется динамически самим процессором.

Следующие методы позволяют достичь суперскалярности:

+ **Внеочередное исполнение команд**. Инструкция попадает в исполнительный модуль не в порядке их следования, а по готовности к выполнению. Это увеличивает загруженность процессора. Эффективность метода тем выше, чем длиннее конвейер и чем больше разница в скорости между памятью и процессором.
+ **Переименование регистров**. Несколько команд могут использовать один и тот же регистр для входных или выходных данных. Такая зависимость по регистру может быть ложной - команды не зависят друг от друга, но их параллельное исполнение невозможно. В таких случаях часто используемые архитектурные регистры (часто используемые) заменяются физическими (их намного больше).
+ **Объединение нескольких команд в одну**.
+ **Предсказание переходов**. Предсказание переходов позволяет сократить время простоя конвейера за счет его загрузки командами, которые должны выполниться после выполнения инструкции условного перехода. Обычно, точность предсказания переходов достигает 90%, что увеличивает загрузку конвейера. В случае, если предсказание было выполнено неверно, конвейер сбрасывается и в него загружается другие команды.
+ **Конвейеризация**. Команда может быть разбита на пять последовательно выполняемых стадий. Это получение инструкции (IF), раскодирование инструкции (ID), выполнение (EX), доступ к памяти (MEM), запись в регистр (WB). Вместо ожидания окончания одной команды, можно выполнить одновременно пять команд, каждая из которых находится в одной из стадии выполнения.
+ **Одновременная многопоточность** (SMT). Одновременное исполнение команд нескольких независимых потоков в каждом цикле на параллельных конвейерах суперскалярного процессора.

Блок суперскалярного процессора, ответственный за распределение команд по функциональным блокам, называется диспетчером инструкций. Этот блок является сложным узлом и от его эффективности зависит производительность всего процессора. Архитектура процессора, которая отказывается от динамического планирования команд в пользу более раннего планирования, называется VLIW.

**VLIW-архитектура** (Very Long Instruction Word) &mdash; архитектура процессора с несколькими вычислительными устройствами. Одна команда процессора содержит несколько операций, которые загружают независимой работой несколько вычислительных устройств. Сборкой команд в пакеты занимается компилятор. Отсутствие диспетчера инструкций упрощает процессор, что уменьшает энергопотребление. VLIW можно считать продолжением RISC, но расширенного несколькими вычислительными модулями. Сложность планирования инструкций переходит на компилятор. Существуют два недостатка этой архитектуры. Первый - это зависимость команд от количества вычислительных устройств. Программа, собранная для одного типа процессора, не запустится на другом, елси количество вычислительных блоков будет другой. Второй недостаток - это непредсказуемая задержка загрузки данных не позволяет эффективно загрузить вычислительные блоки. Решение этих недостатков создало архитектуру EPIC.

**Архитектура EPIC** &mdash; это развитие VLIW, реализованное в процессорах Intel Itanium. Несколько инструкций объединяются в бандлы (bundle). Бандл может иметь стоповый бит, который обозначает, что следующая группа зависит от результатов работы данной группы (исправление первого недостатка VLIW). Для того, чтобы к моменту выполнения команды данные были в кеше, используется программная предподкачка (исправление второго недостатка VLIW).

# Оценка эффективности параллельных программ

**Ускорение** вычисляется по формуле $S_p=T_1/T_p$. Здесь $T_1$ - время исполнения исходной программы, $T_p$ - время исполнения программы на $p$ процессорах. В идеальном случае, при отсутствии накладных расходов, ускорение равно $p$.

**Загруженность (или эффективность распараллеливания)** вычисляется по формуле $S_p/p=T_1/pT_p$ показывает долю использования процессоров. В идеальном случае равна 1 или 100%. Нагляднее загруженность показывает график зависимости от числа процессоров.

**Закон Амдала** &mdash; ограничение роста поизводительности вычислительной системы с увеличением количества вычислителей. Прирост эффективности вычислений зависит от алгоритма задачи и ограничен сверху. Ограничение связано с передачей данных между вычислителями, нераспараллеливаемыми участками задач. Это накладывает ограничение на масштабируемость вычислений. С определенного момента добавление новых узлов в систему будет увеличивать время расчета задачи. *Пример задачи* на 100 секунд выполнения с 5 секундами последовательной части.

# Программные средства распараллеливания

**OpenMP** &mdash; открытый стандарт для распараллеливания программ на языках программирования C, C++, Fortran. Позволяет программировать многопоточные приложения на многопроцессорных системах с общей памятью. Он содержит набор директив компилятора, библиотечные функции и переменные окружения. С 2008 года действует стандарт OpenMP 3.0.

```c++
#include <omp.h>

omp_set_num_threads( 4 )
double sum = 0.0;

#pragma omp parallel for
for( int i = 0; i < 1000; ++i )
{
    double x = Calc( i );
    
    #pragma omp critical
    sum += x;
}
```

**OpenACC** &mdash; программный стандарт для параллельного программирования гетерогенных вычислительных устройств - устройств, использующих центральный и графический процессоры. Использует директивы компилятора для указания какие участки кода где и как выполнять. По аналогии с OpenMP

```c++
#include <openacc.h>

#pragma acc kernels loop independent
for( int i = 0; i < n; ++i )
{
    for( int j = 0; j < n; ++j )
    {
        for( int k = 0; k < n; ++k )
            c[i][j] += a[i][k]*b[k][j];
    }
}
```

**OpenCL** &mdash; открытый стандарт и фреймворк для написания параллельных программ, связанных с параллельными вычислениями на различных графических и центральных процессорах, а также FPGA. Состоит из урезанного языка программирования C99 с набором библиотечных функций.

[Введение](https://habr.com/en/post/72650/) в OpenCL на русском, [пример](https://habr.com/en/post/261323/) работающей программы на С++.

```c++
#include <CL/cl.h>

/* получить доступные платформы */
ret = clGetPlatformIDs(1, &platform_id, &ret_num_platforms);

/* получить доступные устройства */
ret = clGetDeviceIDs(platform_id, CL_DEVICE_TYPE_DEFAULT, 1, &device_id, &ret_num_devices);

/* создать контекст */
context = clCreateContext(NULL, 1, &device_id, NULL, NULL, &ret);

/* создаем команду */
command_queue = clCreateCommandQueue(context, device_id, 0, &ret);

/* создать бинарник из кода программы */
program = clCreateProgramWithSource(context, 1, (const char **)&source_str, (const size_t *)&source_size, &ret);

/* скомпилировать программу */
ret = clBuildProgram(program, 1, &device_id, NULL, NULL, NULL);

/* создать кернел */
kernel = clCreateKernel(program, "test", &ret);
```

**CUDA** (Compute Unified Device Architecture) &mdash; программно-аппаратная архитектура параллельных вычислений от Nvidia. Для реализации параллельных программ используется CUDA SDK. В основе лежит язык программирования, похожий на C, но имеющий некоторые особенности. 

[Пример кода](https://sporgalka.blogspot.com/2011/10/cuda.html)

```c++
__global__ void add( int *a, int *b, int *c ) {
    *c = *a + *b;
}

int main( void ) {
    int a, b, c;                   // host копии a, b, c
    int *dev_a, *dev_b, *dev_c;    // device копии of a, b, c
    int size = sizeof( int );      
    //выделяем память для device копий для a, b, c
    cudaMalloc( (void**)&dev_a, size );
    cudaMalloc( (void**)&dev_b, size );
    cudaMalloc( (void**)&dev_c, size );
    a = 2;
    b = 7;
    // копируем ввод на device
    cudaMemcpy( dev_a, &a, size, cudaMemcpyHostToDevice );
    cudaMemcpy( dev_b, &b, size, cudaMemcpyHostToDevice );
    // запускаем add() kernel на GPU, передавая параметры
    add<<< 1, 1 >>>( dev_a, dev_b, dev_c );
    // copy device result back to host copy of c
    cudaMemcpy( &c, dev_c, size, cudaMemcpyDeviceToHost );
    cudaFree( dev_a );
    cudaFree( dev_b );
    cudaFree( dev_c );
    return 0;
}
```

**MPI** (Message Passing Interface) &mdash; программный интерфейс для передачи данных, который позволяет обмениваться сообщениями между процессами, выполняющими одну задачу. MPI ориентирован на системы с распределенной памятью, когда затраты на передачу данных велики.

[Пример кода](http://mpitutorial.com/tutorials/mpi-hello-world/)

```c++
#include <mpi.h>
#include <stdio.h>

int main(int argc, char** argv) {
    // Initialize the MPI environment
    MPI_Init(NULL, NULL);

    // Get the number of processes
    int world_size;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);

    // Get the rank of the process
    int world_rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

    // Get the name of the processor
    char processor_name[MPI_MAX_PROCESSOR_NAME];
    int name_len;
    MPI_Get_processor_name(processor_name, &name_len);

    // Print off a hello world message
    printf("Hello world from processor %s, rank %d out of %d processors\n",
           processor_name, world_rank, world_size);

    // Finalize the MPI environment.
    MPI_Finalize();
}
```



# Использованные источники

1. https://edu.petrsu.ru/files/upload/4571_1463587331.pdf

2. https://wikipedia.org

   

